---
title: "exp1"
format: html
editor: visual
---

### Load packages

```{r}
# List of packages
packages <- c("corrr", "ggplot2", "lme4", "lmerTest", "FactoMineR", "grid","png","cowplot","magick", "ggimage","dplyr","tidyverse","knitr", "tidytext", "forcats")


```

```{r}

install_and_load_packages <- function(packages) {
  for(package in packages) {
    if(!require(package, character.only = TRUE)) {
      install.packages(package, dependencies = TRUE)
      library(package, character.only = TRUE)
    }
  }
}

# Call the function with the list of packages
install_and_load_packages(packages)

```

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

## Set working directories

```{r}
current_dir<- getwd()

image_dir <- paste0(current_dir, "/../images/")
data_dir <-paste0(current_dir,'/../data/')
data_files <-list.files(path = data_dir, full.names = TRUE, pattern="*.csv")
print(paste0('we have data from ',length(data_files),' participants'))



```

## Load in data and filter our texture ids

```{r}

df_list <- lapply(data_files, read.csv)

experiment_data_raw<- bind_rows(df_list)
ratings_trials<- experiment_data_raw %>% filter(trial_type== 'html-slider-response')
ratings_trials_exp<- ratings_trials %>% filter(practiceTrial != 'true')

ratings_trials_exp$response<- as.numeric(ratings_trials_exp$response)
ratings_trials_exp$response<- (ratings_trials_exp$response + 200)/400



extract_texture_id<-function(filename){
  f<- str_split_i(filename,"/",2)
  f<- str_split_i(f,".png",1)
  return(f)
  
}

ratings_trials_exp$texture <- lapply(ratings_trials_exp$texture_id,extract_texture_id)

#read in key for condition_num
concept_condition_num <- read_csv("/Users/annachinni/Documents/GitHub/visual_textures_associations/analysis/../groups/concept_groups.csv")
#reset condition_num
ratings_trials_exp <- ratings_trials_exp %>%
  select(-condition_num) %>%  # Remove old condition_num
  left_join(concept_condition_num, by = "concept")  # Merge based on concepts


unique_sona_ids <- unique(ratings_trials_exp$sona_id)
cat(paste(unique_sona_ids, collapse = ", "))
```

```{r}
t<-ratings_trials_exp %>%
  group_by(condition_num) %>%
  summarise(unique_subjects = n_distinct(subject_id))

concept_groups <- ratings_trials_exp %>% 
  select(concept, condition_num) %>% 
  distinct() %>% 
  arrange(condition_num)
```

```{r}
colnames(t) <- c("condition_num", "unique_subjects")
t2 <- as.data.frame(t)

```

```{r}
num_ratings <- ratings_trials_exp %>%
  count(concept) %>%
  arrange(desc(n))

#write.csv(num_ratings, "num_ratings.csv", row.names = FALSE)
```

```{r}
#adjusting ratings trials exp for only first rating of a concept per participant
ratings_trials_exp <- ratings_trials_exp %>% 
  group_by(subject_id, concept, texture) %>%
  filter(trial_index == min(trial_index)) %>%
  ungroup()

num_ratings <- ratings_trials_exp %>%
  count(subject_id, concept) %>%
  arrange(desc(n))
```

## Group and pivot into a concept x texture matrix

```{r}
ratings_trial_grouped <- ratings_trials_exp %>% group_by(texture, concept) %>%summarise(mean_rating = mean(response)) 


ratings_trials_grouped_wide<-ratings_trial_grouped %>%
  pivot_wider(names_from = texture, values_from = mean_rating)

ratings_trials_grouped_wide
```

```{r}
### compute matrix rank
SVD <- svd(ratings_trials_grouped_wide[,2:ncol(ratings_trials_grouped_wide)])$d > 1e-10
mat_rank <- sum(SVD)
print(mat_rank)
```

#Plotting

```{r}
#texture paths
texture_paths <- ratings_trials_exp %>% 
  select(texture, texture_id) %>% 
  distinct()

texture_paths <- texture_paths %>%
  mutate(texture_id = file.path(image_dir, basename(texture_id)))

#setting up images for plotting 
# Define output directory for cropped images
output_dir <- paste0(current_dir, "/../cropped_images/")
dir.create(output_dir, showWarnings = FALSE)  # Create directory if doesn't exist 

# Function to crop an image
crop_image <- function(image_path, output_dir) {
  # Read image
  img <- image_read(image_path)
  
  # Get image dimensions
  img_info <- image_info(img)
  width <- img_info$width
  height <- img_info$height

  # Define crop size (adjust as needed, e.g., center 50% of the image)
  crop_width <- floor(0.04 * width)
  crop_height <- floor(0.15 * height)

  # Crop the image (centered)
  cropped_img <- image_crop(img, geometry = paste0(crop_width, "x", crop_height, "+", floor(0.25 * width), "+", floor(0.25 * height)))

  # Define new filename
  new_filename <- file.path(output_dir, basename(image_path))

  # Save cropped image
  image_write(cropped_img, new_filename)
  
  return(new_filename)
}

# Apply cropping to all texture images
texture_paths <- texture_paths %>%
  mutate(cropped_texture_id = sapply(texture_id, crop_image, output_dir = output_dir))
```

```{r}
#add to main data frame
ratings_trials_exp <- left_join(ratings_trials_exp, texture_paths, by = "texture")

#prep plotting data
#averages data
averages <- ratings_trials_exp %>% 
  group_by(concept, texture, cropped_texture_id) %>% 
  summarize(AverageRatings = mean(response), .groups = "drop")

averages$texture <- sapply(averages$texture, as.character)
averages$texture <- as.factor(averages$texture)

#data for hill plots
averages_hills <- averages %>% 
  group_by(concept) %>% 
  arrange(AverageRatings, .by_group = TRUE) %>% 
  mutate(texture = reorder_within(texture, AverageRatings, concept)) %>% 
  ungroup()
```

# Peaky Plots

```{r}
#plot
ggplot(averages, aes(x = texture, y = AverageRatings)) +
  geom_image(aes(image = cropped_texture_id, y = -0.1), size = 0.05) +  # Place images under bars
  geom_bar(stat = "identity", fill = "white", color = "black", size = 0.1) +
  facet_wrap(~ concept) +
  labs(title = "Average Association Ratings by Texture",
       x = "Texture",
       y = "Average Rating") +
  scale_fill_identity() +  
  theme_minimal() +
  #scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  #scale_y_continuous(limits = c(-0.1, 1), expand = c(0, 0)) +  # Adjust y-axis for images
  scale_y_continuous(
    limits = c(-0.2, 1),  # Lower limit for images
    expand = c(0, 0),  
    sec.axis = dup_axis()  # Forces axis to stay at y = 0
  ) +
  # Add a manual x-axis at y = 0
  geom_hline(yintercept = 0, color = "black", size = 0.8) +  # This is now your x-axis
  theme(
    axis.line.y = element_line(color = "black"),
    axis.text.x = element_blank(),
    panel.background = element_blank(),   
    panel.grid.major = element_blank(),      
    panel.grid.minor = element_blank(),      
    plot.background = element_blank()
  )
```

# Hill Plots

```{r}
#plot
hill_plots <- ggplot(averages_hills, aes(x = texture, y = AverageRatings)) +
  geom_image(aes(image = cropped_texture_id, y = -0.1), size = 0.05) +  # Place images under bars
  geom_bar(stat = "identity", fill = "white", color = "black", size = 0.1) +
  facet_wrap(~ concept, scales = "free_x") +
  labs(title = "Average Association Ratings by Texture",
       x = "Texture",
       y = "Average Rating") +
  scale_fill_identity() + 
  scale_x_reordered() +
  theme_minimal() +
  #scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.1, 1), expand = c(0, 0)) +  # Adjust y-axis for images
  scale_y_continuous(
    limits = c(-0.2, 1),  # Lower limit for images
    expand = c(0, 0),  
    sec.axis = dup_axis()  # Forces axis to stay at y = 0
  ) +
  # Add a manual x-axis at y = 0
  geom_hline(yintercept = 0, color = "black", size = 0.8) +  # This is now your x-axis
  theme(
    #axis.line.x = element_line(color = "black"),
    axis.line.y = element_line(color = "black"),
    axis.text.x = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    plot.background = element_blank()
  )

hill_plots
```

Save plot
```{r}
#save the plot
ggsave(filename = "hill_plots.pdf", plot = hill_plots, width = 8, height = 4)
```

# Individual Plots

```{r}
# Set absolute path for the output folder
plot_output_dir <- "/../plots"
dir.create(plot_output_dir, showWarnings = FALSE)  # Create directory if doesn't exist 

unique_concepts <- unique(averages_hills$concept)

for (concept in unique_concepts) {
  # filter to current concept
  concept_data <- averages_hills %>%  filter(concept == !!concept)
  
  #create plot
  plot <- ggplot(concept_data, aes(x = texture, y = AverageRatings)) +
  geom_image(aes(image = cropped_texture_id, y = -0.1), size = 0.05) +  # Place images under bars
  geom_bar(stat = "identity", fill = "white", color = "black", size = 0.1) +
  labs(title = paste("Average Association Ratings for", concept),
       x = "Texture",
       y = "Average Rating") +
  scale_fill_identity() + 
  theme_minimal() +
  #scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.1, 1), expand = c(0, 0)) +  # Adjust y-axis for images
  scale_y_continuous(
    limits = c(-0.2, 1),  # Lower limit for images
    expand = c(0, 0),  
    sec.axis = dup_axis()  # Forces axis to stay at y = 0
  ) +
  # Add a manual x-axis at y = 0
  geom_hline(yintercept = 0, color = "black", size = 0.8) +  # This is now your x-axis
  theme(
    #axis.line.x = element_line(color = "black"),
    axis.line.y = element_line(color = "black"),
    axis.text.x = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    plot.background = element_blank()
  )
  
  
  #save plot to a file
  file_name <- paste0("hill_plot_", gsub("[^a-zA-Z0-9]", "_", concept), ".pdf")
  file_path <- file.path(plot_output_dir, file_name)  # Save inside the specified folder
  
  #save the plot
  ggsave(filename = file_name, plot = plot, width = 8, height = 4)
}

# Print confirmation message
print(paste0("Plots saved to: ", plot_output_dir))

```

# Doubles

```{r}
#semantic distance function 
get_semantic_distance <- function(concept1, concept2) {
  # Helper function for calculating the standard deviation
  sig <- function(xi) {
    return(max(1.4 * xi * (1 - xi), 0.1))
  }
  
  # Extract values from input vectors
  x1 <- concept1[1]
  x2 <- concept1[2]
  x3 <- concept2[1]
  x4 <- concept2[2]
  
  # Calculate semantic distance
  num <- (x1 + x4) - (x2 + x3)
  denom <- sqrt(sig(x1)^2 + sig(x2)^2 + sig(x3)^2 + sig(x4)^2)
  prob_dx_gt_zero <- pnorm(num / denom)
  prob_dx_lt_zero <- 1 - prob_dx_gt_zero
  semantic_distance <- abs(prob_dx_gt_zero - prob_dx_lt_zero)
  
  # Global assignment (1 = outer vs. 0 = inner edges)
  assignDiff <- (x1 + x4) - (x2 + x3)
  if (assignDiff > 0) {
    assign <- 1
  } else if (assignDiff < 0) {
    assign <- 0
  } else {
    assign <- NA
  }
  
  return(list(semantic_distance = semantic_distance, assign = assign))
}
```

```{r}
#Generate all texture pairs
textures <- unique(ratings_trials_exp$texture)
texture_pairs <- combn(textures, 2, simplify = FALSE)
```

```{r}
# Define all possible sets of 8 textures
# Generate all combinations of 8 textures chosen from the 53 available
texture_combinations_53_choose_8 <- combn(textures, 8, simplify = FALSE)

# View the first few combinations
head(texture_combinations_53_choose_8)
```

```{r}
# Register the parallel backend (using all but 1 core)
num_cores <- detectCores() - 1  # Use one less core than available
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Function to generate 53 choose 8 combinations in parallel
generate_combinations_parallel <- function(textures) {
  # Get indices instead of splitting textures
  texture_indices <- seq_along(textures)
  
  # Generate all index combinations of 53 choose 8 (VERY LARGE)
  index_combinations <- combn(texture_indices, 8, simplify = FALSE)

  # Determine chunk size dynamically based on core count
  chunk_size <- ceiling(length(index_combinations) / num_cores)

  # Split index combinations into chunks
  index_chunks <- split(index_combinations, ceiling(seq_along(index_combinations) / chunk_size))
  
  # Use foreach to parallelize the combination generation
  combinations <- foreach(chunk = index_chunks, .combine = c, .packages = "base") %dopar% {
    lapply(chunk, function(idx) textures[idx])  # Convert indices back to texture names
  }
  
  # Stop the parallel cluster
  stopCluster(cl)
  
  return(combinations)
}

# Generate combinations in parallel
textures <- unique(ratings_trials_exp$texture)  # Get your texture list
start_time <- Sys.time()
combinations_53_choose_8_parallel <- generate_combinations_parallel(textures)
end_time <- Sys.time()

# Print runtime
print(paste("Time taken:", end_time - start_time))

# Check the first few combinations
head(combinations_53_choose_8_parallel)
```

```{r}
#practice with fruits and vegetables
```

```{r}
#practice with animals
animals <- ratings_trials_exp %>% 
  filter(concept == c("lion", "bird", "frog", "fish", "bear")) %>% 
  group_by(concept, texture) %>% 
  summarize(AverageRatings = mean(response), .groups = "drop")

# Generate all concept pairs
animal_concepts <- unique(animals$concept)
animal_concept_pairs <- combn(animal_concepts, 2, simplify = FALSE) 

animals_reshaped <- animals %>%
  spread(key = concept, value = AverageRatings)

# Initialize a dataframe to store results
semantic_distances_df <- data.frame(
  concept1 = character(),
  concept2 = character(),
  texture1 = character(),
  texture2 = character(),
  semantic_distance = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each concept pair
for (pair in animal_concept_pairs){
  # Loop through each combination of two textures
  for (textures in texture_pairs) {
    texture1 <- as.character(textures[[1]])
    texture2 <- as.character(textures[[2]])
    
    # Ensure texture1 is not equal to texture2 (redundant check, but explicit)
    if (texture1 != texture2) {
      # Extract ratings
    concept1_texture1_rating <- as.numeric(animals_reshaped[animals_reshaped$texture == texture1, pair[1]]) # X1
    concept1_texture2_rating <- as.numeric(animals_reshaped[animals_reshaped$texture == texture2, pair[2]]) # X4
    concept2_texture1_rating <- as.numeric(animals_reshaped[animals_reshaped$texture == texture1, pair[2]]) # X2
    concept2_texture2_rating <- as.numeric(animals_reshaped[animals_reshaped$texture == texture2, pair[1]]) # X3
    
    # Ensure no missing values
      if (all(!is.na(c(concept1_texture1_rating, concept2_texture2_rating, 
                        concept2_texture1_rating, concept1_texture2_rating)))) {
        
        # Compute semantic distance
        result <- get_semantic_distance(
          c(concept1_texture1_rating, concept2_texture2_rating),  # (x1, x3)
          c(concept2_texture1_rating, concept1_texture2_rating)   # (x2, x4)
        )
        
        # Append result to dataframe
        semantic_distances_df <- rbind(semantic_distances_df, 
          data.frame(
            concept1 = pair[1], 
            concept2 = pair[2], 
            texture1 = texture1, 
            texture2 = texture2, 
            semantic_distance = result$semantic_distance
          )
        )
      }
    }
  }
}

# Add identifier column with all the info
semantic_distances_df <- semantic_distances_df %>%
  mutate(identifier = paste(concept1, concept2, texture1, texture2, sep = "_"))
```

```{r}
# Function to plot semantic distances and calculate MSE
plot_semantic_distances <- function(df, concept1, concept2) {
  # Filter for the given concept pair
  filtered_df <- df %>%
    filter((concept1 == !!concept1 & concept2 == !!concept2) | 
           (concept1 == !!concept2 & concept2 == !!concept1))

  # Get unique textures from the filtered data
  unique_textures <- unique(c(filtered_df$texture1, filtered_df$texture2))

  # Select 8 random textures (adjust if fewer than 8 available)
  selected_textures <- sample(unique_textures, min(8, length(unique_textures)))

  # Generate all possible texture pairs (8 choose 2 = 28 pairs)
  texture_pairs <- combn(selected_textures, 2, simplify = FALSE)

  # Extract semantic distances for the selected texture pairs
  selected_distances <- filtered_df %>%
    filter((texture1 %in% selected_textures) & (texture2 %in% selected_textures))

  # Ensure only the exact (8 choose 2) combinations are included
  selected_distances <- selected_distances %>%
    filter(paste(texture1, texture2, sep = "_") %in% 
             sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_")) |
           paste(texture2, texture1, sep = "_") %in% 
             sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_"))) %>%
    arrange(semantic_distance)  # Sort by semantic distance

  # Create label column for texture pairs, spacing only the labels
  selected_distances <- selected_distances %>%
    mutate(label = paste(texture1, texture2, sep = " vs "),
           y_label_position = row_number() * 0.2)  # Space labels only

  # Generate perfect semantic distance values between 0.1 and 0.9
  perfect_distances <- seq(0.1, 0.9, length.out = 28)

  # Calculate Mean Squared Error (MSE) between actual and perfect distances
  mse <- mean((selected_distances$semantic_distance - perfect_distances)^2)
  print(paste("Mean Squared Error (MSE):", mse))

  # First plot: Bar plot for texture pairs
  bar_plot <- ggplot(selected_distances, aes(x = paste(texture1, texture2, sep = " vs "), y = semantic_distance)) +
    geom_col(fill = "steelblue") +
    labs(title = paste("Semantic Distances for", concept1, "vs", concept2),
         x = "Texture Pair",
         y = "Semantic Distance") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Second plot: Scatter plot with spaced-out labels
  scatter_plot <- ggplot(selected_distances, aes(x = semantic_distance, y = 3)) +  # Points stay at y = 0
    geom_point(size = 3, color = "red", alpha = 0.7) +  # Red points for visibility
    xlim(0, 1.4) +  # Ensure space for labels on the right
    labs(title = paste("Semantic Distance Distribution for", concept1, "vs", concept2),
         x = "Semantic Distance",
         y = "") +  # Remove y-axis label
    theme_minimal() +
    theme(axis.text.y = element_blank(),  # Remove y-axis labels
          axis.ticks.y = element_blank(),  # Remove y-axis ticks
          panel.grid.major.y = element_blank(),  # Remove horizontal grid lines
          panel.grid.minor.y = element_blank()) +  # Remove minor horizontal grid lines
    geom_text(aes(x = 1.05, label = label, y = y_label_position), hjust = 0, size = 4)  # Space labels only

  # # Plot for the perfect distance comparison
  # perfect_plot <- ggplot(data.frame(x = perfect_distances, y = rep(1, 28)), aes(x = x, y = y)) +
  #   geom_point(color = "blue", size = 3, alpha = 0.7) +
  #   xlim(0, 1.1) +
  #   labs(title = "Perfect Semantic Distance Distribution (0.1 to 0.9)",
  #        x = "Semantic Distance",
  #        y = "") +
  #   theme_minimal() +
  #   theme(axis.text.y = element_blank(),
  #         axis.ticks.y = element_blank(),
  #         panel.grid.major.y = element_blank(),
  #         panel.grid.minor.y = element_blank())

  # Show all plots
  print(bar_plot)
  print(scatter_plot)
  #print(perfect_plot)
}

# Example: Plot for "bear" vs "bird"
plot_semantic_distances(semantic_distances_df, "bear", "bird")
```

```{r}

# Function to plot semantic distances and calculate MSE for every combination of 8 textures
plot_semantic_distances <- function(df) {
  # Get unique textures from the data
  unique_textures <- unique(c(df$texture1, df$texture2))
  
  # Generate all combinations of 8 textures (8 choose 2 combinations)
  texture_combinations <- combn(unique_textures, 8, simplify = FALSE)
  
  # Initialize an empty dataframe to store MSE results
  mse_results <- data.frame(
    texture_combination = character(),
    mse = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Initialize an empty list to store the plots
  plots <- list()
  
  # Loop through each combination of 8 textures
  for (texture_set in texture_combinations) {
    # Generate all possible texture pairs (8 choose 2 = 28 pairs)
    texture_pairs <- combn(texture_set, 2, simplify = FALSE)
    
    # Extract semantic distances for the selected texture pairs
    selected_distances <- df %>%
      filter((texture1 %in% texture_set) & (texture2 %in% texture_set))
    
    # Ensure only the exact combinations are included
    selected_distances <- selected_distances %>%
      filter(paste(texture1, texture2, sep = "_") %in% 
               sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_")) |
             paste(texture2, texture1, sep = "_") %in% 
               sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_"))) %>%
      arrange(semantic_distance)  # Sort by semantic distance
    
    # Create label column for texture pairs
    selected_distances <- selected_distances %>%
      mutate(label = paste(texture1, texture2, sep = " vs "),
             y_label_position = row_number() * 0.2)  # Space labels only
    
    # Generate perfect semantic distance values between 0.1 and 0.9
    perfect_distances <- seq(0.1, 0.9, length.out = 28)
    
    # Calculate Mean Squared Error (MSE) between actual and perfect distances
    mse <- mean((selected_distances$semantic_distance - perfect_distances)^2)
    
    # Save MSE results with the texture set
    mse_results <- rbind(mse_results, 
                         data.frame(texture_combination = paste(texture_set, collapse = "_"),
                                    mse = mse))
    
    # Generate plots for this combination of textures
    bar_plot <- ggplot(selected_distances, aes(x = paste(texture1, texture2, sep = " vs "), y = semantic_distance)) +
      geom_col(fill = "steelblue") +
      labs(title = paste("Semantic Distances for", paste(texture_set, collapse = "_")),
           x = "Texture Pair",
           y = "Semantic Distance") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    scatter_plot <- ggplot(selected_distances, aes(x = semantic_distance, y = 0)) +  # Points stay at y = 0
      geom_point(size = 3, color = "red", alpha = 0.7) +  # Red points for visibility
      xlim(0, 1.1) +  # Ensure space for labels on the right
      labs(title = paste("Semantic Distance Distribution for", paste(texture_set, collapse = "_")),
           x = "Semantic Distance",
           y = "") +  # Remove y-axis label
      theme_minimal() +
      theme(axis.text.y = element_blank(),  # Remove y-axis labels
            axis.ticks.y = element_blank(),  # Remove y-axis ticks
            panel.grid.major.y = element_blank(),  # Remove horizontal grid lines
            panel.grid.minor.y = element_blank()) +  # Remove minor horizontal grid lines
      geom_text(aes(x = 1.05, label = label, y = y_label_position), hjust = 0, size = 4)  # Space labels only
    
    # perfect_plot <- ggplot(data.frame(x = perfect_distances, y = rep(1, 28)), aes(x = x, y = y)) +
    #   geom_point(color = "blue", size = 3, alpha = 0.7) +
    #   xlim(0, 1.1) +
    #   labs(title = "Perfect Semantic Distance Distribution (0.1 to 0.9)",
    #        x = "Semantic Distance",
    #        y = "") +
    #   theme_minimal() +
    #   theme(axis.text.y = element_blank(),
    #         axis.ticks.y = element_blank(),
    #         panel.grid.major.y = element_blank(),
    #         panel.grid.minor.y = element_blank())
    
    # Store the plots
    plots[[paste(texture_set, collapse = "_")]] <- list(bar_plot, scatter_plot)
  }
  
  # Combine all plots into facets
  all_plots <- do.call(gridExtra::grid.arrange, c(plots, ncol = 2))
  
  # Return the MSE results dataframe and the combined plot
  list(mse_results = mse_results, combined_plot = all_plots)
}

# Example: Run the function
result <- plot_semantic_distances(semantic_distances_df)

# View the MSE results
print(result$mse_results)
```

```{r}
library(doParallel)
library(foreach)
```

```{r}
# Function to plot semantic distances and calculate MSE for every combination of 8 textures
plot_semantic_distances_parallel <- function(df) {
  # Get unique textures from the data
  unique_textures <- unique(c(df$texture1, df$texture2))
  
  # Generate all combinations of 8 textures (8 choose 2 combinations)
  texture_combinations <- combn(unique_textures, 8, simplify = FALSE)
  
  # Initialize an empty dataframe to store MSE results
  mse_results <- data.frame(
    texture_combination = character(),
    mse = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Set up parallel backend to use multiple cores
  num_cores <- detectCores() - 1  # Use one less than the total number of cores
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)
  
  # Run the loop in parallel using foreach
  results <- foreach(texture_set = texture_combinations, .combine = rbind, .packages = c("dplyr", "ggplot2")) %dopar% {
    # Generate all possible texture pairs (8 choose 2 = 28 pairs)
    texture_pairs <- combn(texture_set, 2, simplify = FALSE)
    
    # Extract semantic distances for the selected texture pairs
    selected_distances <- df %>%
      filter((texture1 %in% texture_set) & (texture2 %in% texture_set))
    
    # Ensure only the exact combinations are included
    selected_distances <- selected_distances %>%
      filter(paste(texture1, texture2, sep = "_") %in% 
               sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_")) |
             paste(texture2, texture1, sep = "_") %in% 
               sapply(texture_pairs, function(x) paste(x[1], x[2], sep = "_"))) %>%
      arrange(semantic_distance)  # Sort by semantic distance
    
    # Create label column for texture pairs
    selected_distances <- selected_distances %>%
      mutate(label = paste(texture1, texture2, sep = " vs "),
             y_label_position = row_number() * 0.2)  # Space labels only
    
    # Generate perfect semantic distance values between 0.1 and 0.9
    perfect_distances <- seq(0.1, 0.9, length.out = 28)
    
    # Calculate Mean Squared Error (MSE) between actual and perfect distances
    mse <- mean((selected_distances$semantic_distance - perfect_distances)^2)
    
    # Return results to be combined later
    mse_result <- data.frame(texture_combination = paste(texture_set, collapse = "_"),
                             mse = mse)
    
    return(mse_result)
  }
  
  # Stop the parallel cluster
  stopCluster(cl)
  
  # Combine results into a final dataframe
  mse_results <- results
  
  # Return the MSE results dataframe
  return(mse_results)
}

# Example: Run the parallel function
mse_results <- plot_semantic_distances_parallel(semantic_distances_df)

# View the MSE results
print(mse_results)
```

# Rating Stability

```{r}
#estimating data stability using split half correlations of current data
compute_all_split_correlations <- function(ratings_df, t2, condition_nums, n_iterations) {
  all_correlation_results <- list()
  counter <- 1  # To track the list index
  
  for (condition in condition_nums) {
    max_size <- t2 %>% filter(condition_num == condition) %>% pull(unique_subjects) / 2
    
    for (subsample_size in 1:max_size) {  # Iterate over different sample sizes
      for (i in 1:n_iterations) {
        # Randomly select subsample_size subjects for each group
        group1ids <- ratings_df %>%
          filter(condition_num == condition) %>%
          sample_n(subsample_size, replace = FALSE) %>%
          select(subject_id)
        
        group2ids <- ratings_df %>%
          filter(condition_num == condition) %>%
          filter(!subject_id %in% group1ids$subject_id) %>%
          sample_n(subsample_size, replace = FALSE) %>%
          select(subject_id)
        
        # Create data matrices for both groups
        group1df <- ratings_df %>% filter(subject_id %in% group1ids$subject_id)
        group2df <- ratings_df %>% filter(subject_id %in% group2ids$subject_id)
        
        group1mat <- group1df %>%
          group_by(texture, concept) %>%
          summarise(mean_rating = mean(response), .groups = 'keep') %>%
          pivot_wider(names_from = texture, values_from = mean_rating)
        
        group2mat <- group2df %>%
          group_by(texture, concept) %>%
          summarise(mean_rating = mean(response), .groups = 'keep') %>%
          pivot_wider(names_from = texture, values_from = mean_rating)
        
        # Align matrices
        concept_names <- group1mat$concept
        group1mat <- group1mat[order(group1mat$concept), -1]  # Remove concept column
        group2mat <- group2mat[order(group2mat$concept), -1]  # Remove concept column
        
        common_columns <- intersect(colnames(group1mat), colnames(group2mat))
        group1mat <- group1mat[, sort(common_columns)]
        group2mat <- group2mat[, sort(common_columns)]
        
        # Compute correlations
        rowwise_correlations <- mapply(
          function(row1, row2) cor(row1, row2, use = "pairwise.complete.obs"),
          as.data.frame(t(group1mat)), as.data.frame(t(group2mat))
        )
        
        # Store results
        all_correlation_results[[counter]] <- data.frame(
          iteration = i,
          condition_num = condition,
          sample_size = subsample_size,  # Store subsample size
          concept = concept_names,
          correlation = rowwise_correlations
        )
        
        counter <- counter + 1  # Increment index
      }
    }
  }
  
  # Combine all results into a single data frame
  return(bind_rows(all_correlation_results))
}

# Get unique condition numbers from ratings_df
condition_nums <- unique(ratings_trials_exp$condition_num)

library(progressr)
handlers("progress")

# Run the function
all_split_results <- compute_all_split_correlations(ratings_trials_exp, t2, condition_nums, n_iterations = 1000) 
```

```{r}
#Plot Split half correlations

# Summarize data for each concept and sample size
summarize_split_halves <- all_split_results %>%
  group_by(concept, sample_size) %>%
  summarise(
    mean_correlation = mean(correlation, na.rm = TRUE),
    se_correlation = sd(correlation, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

summarize_split_halves <- left_join(summarize_split_halves, concept_groups, by = "concept") %>% 
  arrange(condition_num)

#ggplot
ggplot(summarize_split_halves, aes(x = sample_size, y = mean_correlation, color = concept)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_correlation - se_correlation, ymax = mean_correlation + se_correlation, group = concept),
              alpha = 0.1, fill = "red", color = NA) +
  geom_text(
    data = summarize_split_halves %>%
      group_by(condition_num, concept) %>%
      filter(sample_size == max(sample_size)), # Place label at the largest sample size
    aes(
      x = sample_size,
      y = mean_correlation,
      label = concept
    ),
    hjust = -0.1, # Slightly offset the labels horizontally
    vjust = 0, # Adjust vertical spacing
    size = 3,
    inherit.aes = FALSE # Avoid inheriting other plot aesthetics
  ) +
  # Add vertical lines for current sample sizes from `t2`
  geom_vline(
    data = t2,
    aes(xintercept = unique_subjects/2), 
    linetype = "dashed", 
    color = "black"
  ) +
  labs(
    title = "Mean Reliability Across Sample Sizes for Each Concept and Condition",
    x = "Number of Samples Correlated (Sample Size = Samples Correlated x 2)",
    y = "Mean Reliability"
  ) +
  theme_minimal() +
  facet_wrap(~ condition_num, ncol = 7) + 
  theme(
    legend.position = "none",
    plot.margin = margin(50, 50, 50, 50),
    panel.spacing = unit(3, "lines")) +
  #scale_color_manual(values = custom_colors) +
  coord_cartesian(clip = "off")
```

# Bootstrapping Stuff (if wanted)

```{r}

bootstrap_associations_matrices<- function(ratings_df, sample_size){
  
  group1ids<- ratings_df %>%
  group_by(condition_num) %>%
  sample_n(sample_size,replace = TRUE)%>%select(subject_id)
  
  group2ids <- ratings_df %>%
  # filter(!subject_id %in% group1ids$subject_id) %>%
  group_by(condition_num) %>%
  sample_n(sample_size, replace = TRUE) %>%
  select(subject_id)
  
  group1df<-ratings_df%>%filter(subject_id%in%group1ids$subject_id)
  group2df<-ratings_df%>%filter(subject_id%in%group2ids$subject_id)
  
  group1mat<- group1df%>% group_by(texture, concept) %>%summarise(mean_rating = mean(response),.groups='keep')%>%
  pivot_wider(names_from = texture, values_from = mean_rating)
  group2mat<- group2df%>% group_by(texture, concept) %>%summarise(mean_rating = mean(response),.groups='keep')%>%
  pivot_wider(names_from = texture, values_from = mean_rating)
  
  concept_names <- group1mat$concept # Preserve concept names in correct order for later
  
  group1mat <- group1mat[order(group1mat$concept), -1]  # Remove concept column
  group2mat <- group2mat[order(group2mat$concept), -1]  # Remove concept column
  
  common_columns <- intersect(colnames(group1mat), colnames(group2mat))  # Find common columns
  group1mat <- group1mat[, sort(common_columns)]
  group2mat <- group2mat[, sort(common_columns)]
  
  ### @Anna lol could you just change things below so that we compute concept-wise correlations and return a dataframe of correlations for each concept
  
  ### @Kushin lol yes. I changed things so that the function returns a list with both the mean_correlation and the correlation_df (the correlation_df specifies the correlation value for each concept)
  
  rowwise_correlations <- mapply(function(row1, row2) {
    cor(row1, row2, use = "pairwise.complete.obs")
  }, as.data.frame(t(group1mat)), as.data.frame(t(group2mat)))
  
  # Create dataframe of concept-wise correlations
  correlation_df <- data.frame(
    concept = concept_names,
    correlation = rowwise_correlations
  )
  
  # Compute mean correlation
  mean_correlation <- mean(rowwise_correlations, na.rm = TRUE)
  
  return(list(mean_correlation = mean_correlation, correlation_df = correlation_df))

  
}


```

```{r}
### @Kushin I updated the running code so that it works as before with the updated function 
iters<-numeric()
sample_sizes<-numeric()
mean_rs<-numeric()
correlation_dfs<-list()
for(iter in 1:1000){
for(sample_size in seq(5, 80, by = 5)){
  result <- bootstrap_associations_matrices(ratings_trials_exp, sample_size)
  mean_rs <- rbind(mean_rs, result$mean_correlation)
  correlation_dfs[[length(correlation_dfs) + 1]] <- result$correlation_df
  iters<- rbind(iters, iter)
  sample_sizes<-rbind(sample_sizes, sample_size)

}
}


bootstrap_df <- data.frame(cbind( iters,sample_sizes,mean_rs
))
colnames(bootstrap_df)<- c('iteration','sample_size','mean_reliability')



# Summarize data to calculate mean and standard error for each sample_size
summary_results <-bootstrap_df %>%
    group_by(sample_size) %>%
    summarise(
        se_reliability = sd(mean_reliability, na.rm = TRUE) / sqrt(n()),
        mean_reliability = mean(mean_reliability, na.rm = TRUE),
       
    )

# Create ggplot - this is for overall data - not seperated by group and concept
ggplot(summary_results, aes(x = sample_size, y = mean_reliability)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = mean_reliability - se_reliability,
                  ymax = mean_reliability + se_reliability),
              alpha = 0.1, fill = "red") +
  labs(
    title = "Mean Reliability Across Sample Sizes",
    x = "Number of Samples Correlated (Sample Size = Samples Correlated x 2",
    y = "Mean Reliability"
  ) +
  theme_minimal()
```

```{r}
# Now plots for individual concepts

# Combine all correlation data frames into one with iteration and sample size info
correlation_summary_df <- bind_rows(
  lapply(seq_along(correlation_dfs), function(i) {
    cbind(
      iteration = iters[i],
      sample_size = sample_sizes[i],
      correlation_dfs[[i]]
    )
  })
)

# Summarize data for each concept and sample size
summary_correlation_results <- correlation_summary_df %>%
  group_by(concept, sample_size) %>%
  summarise(
    mean_correlation = mean(correlation, na.rm = TRUE),
    se_correlation = sd(correlation, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Add condition_num
summary_correlation_results <- left_join(summary_correlation_results, concept_groups, by = "concept") %>% 
  arrange(condition_num)


#ggplot
ggplot(summary_correlation_results, aes(x = sample_size, y = mean_correlation, color = concept)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_correlation - se_correlation, ymax = mean_correlation + se_correlation, group = concept),
              alpha = 0.1, fill = "red", color = NA) +
  geom_text(
    data = summary_correlation_results %>%
      group_by(condition_num, concept) %>%
      filter(sample_size == max(sample_size)), # Place label at the largest sample size
    aes(
      x = sample_size,
      y = mean_correlation,
      label = concept
    ),
    hjust = -0.1, # Slightly offset the labels horizontally
    vjust = 0, # Adjust vertical spacing
    size = 3,
    inherit.aes = FALSE # Avoid inheriting other plot aesthetics
  ) +
  # Add vertical lines for current sample sizes from `t2`
  geom_vline(
    data = t2,
    aes(xintercept = unique_subjects/2), 
    linetype = "dashed", 
    color = "black"
  ) +
  labs(
    title = "Mean Reliability Across Sample Sizes for Each Concept and Condition",
    x = "Number of Samples Correlated (Sample Size = Samples Correlated x 2)",
    y = "Mean Reliability"
  ) +
  theme_minimal() +
  facet_wrap(~ condition_num, ncol = 7) + 
  theme(
    legend.position = "none",
    plot.margin = margin(50, 50, 50, 50),
    panel.spacing = unit(3, "lines")) +
  #scale_color_manual(values = custom_colors) +
  coord_cartesian(clip = "off")
```

```{r}
#split half correlations for current sample size
compute_correlations <- function(ratings_df, t2, condition_num, n_iterations = 1000) {
  all_correlation_results <- vector("list", length(condition_num) * n_iterations)
  counter <- 1  # To track the list index
  
  for (condition in condition_num) {
    for (i in 1:n_iterations) {
      # Randomly split subjects into two groups
      group1ids <- ratings_df %>%
        filter(condition_num == condition) %>%
        sample_n(t2 %>% filter(condition_num == condition) %>% pull(unique_subjects) / 2, replace = FALSE) %>%
        select(subject_id)
      
      group2ids <- ratings_df %>%
        filter(condition_num == condition) %>%
        filter(!subject_id %in% group1ids$subject_id) %>%
        sample_n(t2 %>% filter(condition_num == condition) %>% pull(unique_subjects) / 2, replace = FALSE) %>%
        select(subject_id)
      
      # Create data matrices for both groups
      group1df <- ratings_df %>% filter(subject_id %in% group1ids$subject_id)
      group2df <- ratings_df %>% filter(subject_id %in% group2ids$subject_id)
      
      group1mat <- group1df %>%
        group_by(texture, concept) %>%
        summarise(mean_rating = mean(response), .groups = 'keep') %>%
        pivot_wider(names_from = texture, values_from = mean_rating)
      
      group2mat <- group2df %>%
        group_by(texture, concept) %>%
        summarise(mean_rating = mean(response), .groups = 'keep') %>%
        pivot_wider(names_from = texture, values_from = mean_rating)
      
      # Align matrices
      concept_names <- group1mat$concept
      group1mat <- group1mat[order(group1mat$concept), -1]  # Remove concept column
      group2mat <- group2mat[order(group2mat$concept), -1]  # Remove concept column
      
      common_columns <- intersect(colnames(group1mat), colnames(group2mat))
      group1mat <- group1mat[, sort(common_columns)]
      group2mat <- group2mat[, sort(common_columns)]
      
      # Compute correlations
      rowwise_correlations <- mapply(
        function(row1, row2) cor(row1, row2, use = "pairwise.complete.obs"),
        as.data.frame(t(group1mat)), as.data.frame(t(group2mat))
      )
      
      # Store results
      all_correlation_results[[counter]] <- data.frame(
        iteration = i,
        condition_num = condition,
        concept = concept_names,
        correlation = rowwise_correlations
      )
      
      counter <- counter + 1  # Increment index
    }
  }
  
  # Combine all results into a single data frame
  return(bind_rows(all_correlation_results))
}

# Get unique condition numbers from ratings_df
condition_nums <- unique(ratings_trials_exp$condition_num)

# Run the function
all_results <- compute_correlations(ratings_trials_exp, t2, condition_nums, n_iterations = 1000)
```

```{r}
#plot distributions of correlations for each concept in each group
```

```{r}
#plotting

#combine correlation_summary_df with all_results for combined plotting
#first make all_results smaller for r's memory
all_results <- all_results %>% 
  group_by(condition_num, concept) %>% 
  mutate(
    avg_correlation = mean(correlation)
  ) %>% 
  ungroup()

all_results <- all_results %>% 
  select(condition_num, concept, avg_correlation) %>% 
  ungroup()
  
all_results <- all_results %>% 
  distinct()

boostrapping_w_split_half <- merge(summary_correlation_results, all_results, by = "concept", all.x = TRUE)

boostrapping_w_split_half <- boostrapping_w_split_half %>% 
  select(-condition_num.y) %>% 
  rename(condition_num = condition_num.x)

#add t2 for unique_subjects
boostrapping_w_split_half <- left_join(boostrapping_w_split_half, t2, by = "condition_num")

#make sure condition_num is character value 
boostrapping_w_split_half <- boostrapping_w_split_half %>% 
  mutate(
    condition_num = as.character(condition_num)
  )

#use summary results from boostrapping to compare
#create vertical line each group at current sample size
#add point for correlation for each group at current sample size

#ggplot
ggplot(boostrapping_w_split_half, aes(x = sample_size, y = mean_correlation, color = concept)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_correlation - se_correlation, ymax = mean_correlation + se_correlation, group = concept),
              alpha = 0.1, fill = "red", color = NA) +
  geom_text(
    data = boostrapping_w_split_half %>%
      group_by(condition_num, concept) %>%
      filter(sample_size == max(sample_size)), # Place label at the largest sample size
    aes(
      x = sample_size,
      y = mean_correlation,
      label = concept
    ),
    hjust = -0.1, # Slightly offset the labels horizontally
    vjust = 0, # Adjust vertical spacing
    size = 3,
    inherit.aes = FALSE # Avoid inheriting other plot aesthetics
  ) +
  # Add vertical lines for current sample sizes from `t2`
  geom_vline(
    data = boostrapping_w_split_half,
    aes(xintercept = unique_subjects/2), 
    linetype = "dashed", 
    color = "black"
  ) +
  geom_point(
    data = boostrapping_w_split_half,
    aes(x = unique_subjects/2, y = avg_correlation, color = concept, group = concept)
  ) +
  labs(
    title = "Mean Reliability Across Sample Sizes for Each Concept and Condition",
    x = "Number of Samples Correlated (Sample Size = Samples Correlated x 2)",
    y = "Mean Reliability"
  ) +
  theme_minimal() +
  facet_wrap(~ condition_num, ncol = 7) + 
  theme(
    legend.position = "none",
    plot.margin = margin(50, 50, 50, 50),
    panel.spacing = unit(3, "lines")) +
  #scale_color_manual(values = custom_colors) +
  coord_cartesian(clip = "off")

```
